{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn: A Comprehensive Guide\n",
    "\n",
    "Scikit-learn is a powerful and widely-used library for machine learning in Python. It provides simple and efficient tools for data mining and data analysis, making it a valuable resource for building machine learning models.\n",
    "\n",
    "### Table of Contents\n",
    "1. **Introduction to Scikit-learn**\n",
    "2. **Loading Datasets**\n",
    "3. **Preprocessing Data**\n",
    "    - Imputation\n",
    "    - Scaling\n",
    "    - Encoding Categorical Variables\n",
    "    - Binarization\n",
    "4. **Splitting Data**\n",
    "5. **Model Selection and Evaluation**\n",
    "    - Train-Test Split\n",
    "    - Cross-Validation\n",
    "    - Metrics\n",
    "6. **Supervised Learning Algorithms**\n",
    "    - Linear Regression\n",
    "    - Logistic Regression\n",
    "    - Support Vector Machines\n",
    "    - Decision Trees\n",
    "    - Random Forests\n",
    "7. **Unsupervised Learning Algorithms**\n",
    "    - K-Means Clustering\n",
    "    - Principal Component Analysis (PCA)\n",
    "    - Model Pipelines\n",
    "8. **Saving and Loading Models**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Scikit-learn\n",
    "\n",
    "Scikit-learn is a Python library designed for machine learning tasks. It provides simple and efficient tools for data analysis, covering various tasks like classification, regression, clustering, and more.\n",
    "\n",
    "## 2. Loading the Dataset\n",
    "\n",
    "We'll start by loading the Iris dataset. This dataset is commonly used for classification tasks and contains 150 samples with 4 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data   # Features (sepal length, sepal width, petal length, petal width)\n",
    "y = iris.target # Labels (types of iris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: datasets.load_iris() loads the Iris dataset, where X contains the features, and y contains the target labels (types of iris flowers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data\n",
    "\n",
    "Preprocessing involves transforming the data into a suitable format for modeling. Let's look at the most common preprocessing steps.\n",
    "\n",
    "### 3.1 Imputation (Handling Missing Values)\n",
    "\n",
    "Although the Iris dataset doesn't have missing values, it's important to know how to handle them in general. For this, we would use the `SimpleImputer` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data with NaN:\n",
      " [[nan 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "After Imputation:\n",
      " [[5.84832215 3.5        1.4        0.2       ]\n",
      " [4.9        3.         1.4        0.2       ]\n",
      " [4.7        3.2        1.3        0.2       ]\n",
      " [4.6        3.1        1.5        0.2       ]\n",
      " [5.         3.6        1.4        0.2       ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Assuming some missing values (for demonstration)\n",
    "X_with_nan = X.copy()\n",
    "X_with_nan[0, 0] = np.nan  # Introduce a missing value\n",
    "\n",
    "# Imputation (replace missing values with the mean)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X_with_nan)\n",
    "\n",
    "print(\"Original Data with NaN:\\n\", X_with_nan[:5])\n",
    "print(\"After Imputation:\\n\", X_imputed[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: SimpleImputer(strategy='mean') replaces missing values with the mean of the respective column. The fit_transform method applies this transformation to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scaling (Standardization)\n",
    "\n",
    "Features should be on a similar scale for some models (like SVM) to perform well. We can scale the features using 'StandardScaler'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Scaled Data:\n",
      " [[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
      " [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
      " [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
      " [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
      " [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Original Data:\\n\", X[:5])\n",
    "print(\"Scaled Data:\\n\", X_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: StandardScaler standardizes features by removing the mean and scaling to unit variance, ensuring all features are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Encoding Categorical Variables\n",
    "\n",
    "Since the Iris dataset labels are already numerical, we don't need encoding. But if we had categorical features, we'd use `OneHotEncoder` to convert them to a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Labels:\n",
      " [0 0 0 0 0]\n",
      "One-Hot Encoded Labels:\n",
      " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 5 stored elements and shape (5, 3)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example: encoding target labels (though they are already numerical)\n",
    "encoder = OneHotEncoder()\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "print(\"Original Labels:\\n\", y[:5])\n",
    "print(\"One-Hot Encoded Labels:\\n\", y_encoded[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: OneHotEncoder converts categorical labels into a one-hot encoded format, where each category is represented by a binary vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Binarization\n",
    "\n",
    "Binarization is used to threshold features. For example, if you want to create binary features based on a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Binarized Data:\n",
      " [[1. 1. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Initialize the binarizer with a threshold value\n",
    "binarizer = Binarizer(threshold=2.5)\n",
    "\n",
    "# Transform the features\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "\n",
    "print(\"Original Data:\\n\", X[:5])\n",
    "print(\"Binarized Data:\\n\", X_binarized[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `Binarizer(threshold=2.5)` converts features to binary values (0 or 1) based on a threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting Data\n",
    "\n",
    "Before training a model, it's important to split the dataset into training and testing sets. This helps in evaluating the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (120, 4)\n",
      "Testing Data Shape: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data: 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `train_test_split` splits the data into training and testing sets. The `test_size=0.2` means 20% of the data is reserved for testing.\n",
    "\n",
    "## 5. Model Selection and Evaluation\n",
    "\n",
    "Now that the data is preprocessed and split, we can build a model. Let's use a Logistic Regression model as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels:\n",
      " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predicted Labels:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `LogisticRegression` is a simple model used for classification. We fit it to the training data and then use it to predict the labels of the test data.\n",
    "\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "After training the model, we need to evaluate its performance using metrics like accuracy, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "  - `accuracy_score`: Measures the percentage of correct predictions.\n",
    "  - `confusion_matrix`: Shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "  - `classification_report`: Provides precision, recall, and F1-score for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "By following this step-by-step approach, we've covered essential scikit-learn concepts using the Iris dataset. We've handled data loading, preprocessing, splitting, model selection, and evaluation in a way that ensures the entire process runs smoothly without errors. Each function and step was introduced where needed, making the learning experience easier to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Supervised Learning Algorithms\n",
    "\n",
    "### 6.1 Linear Regression\n",
    "\n",
    "Linear Regression is used for predicting continuous values. Although it's not suitable for the Iris dataset (as it's used for classification), I'll show you how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03711379440797686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `LinearRegression` predicts continuous values. We use `mean_squared_error` to evaluate the model's performance.\n",
    "\n",
    "\n",
    "### 6.2 Logistic Regression\n",
    "Logistic Regression is used for binary or multi-class classification problems. We have already used it in our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `LogisticRegression` is used for classification. We evaluate it using accuracy, confusion matrix, and classification report.\n",
    "\n",
    "### 6.3 Support Vector Machines (SVM)\n",
    "\n",
    "SVM is a powerful classification technique that works well with both linear and non-linear boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the SVM model\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"SVM Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `SVC` (Support Vector Classifier) is used for classification. We evaluate it similarly to other classifiers.\n",
    "\n",
    "### 6.4 Decision Trees\n",
    "\n",
    "Decision Trees are a versatile classification and regression method that model data as a series of decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Decision Tree Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `DecisionTreeClassifier` splits the data into branches to make decisions. Performance is evaluated using accuracy.\n",
    "\n",
    "### 6.5 Random Forests\n",
    "\n",
    "Random Forests are an ensemble method that combines multiple decision trees to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Random Forest Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `RandomForestClassifier` creates a forest of decision trees. It improves performance by averaging multiple trees' results.\n",
    "\n",
    "## 7. Unsupervised Learning Algorithms\n",
    "\n",
    "### 7.1 K-Means Clustering\n",
    "K-Means is a clustering algorithm that partitions data into `k` clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      " [[6.85384615 3.07692308 5.71538462 2.05384615]\n",
      " [5.006      3.428      1.462      0.246     ]\n",
      " [5.88360656 2.74098361 4.38852459 1.43442623]]\n",
      "Cluster Labels:\n",
      " [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize and fit the K-Means model\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "clusters = kmeans.predict(X)\n",
    "\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n",
    "print(\"Cluster Labels:\\n\", clusters[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `KMeans` assigns each data point to one of `k` clusters. We use `fit` to learn the clusters and `predict` to assign labels.\n",
    "\n",
    "### 7.2 Principal Component Analysis (PCA)\n",
    "\n",
    "PCA reduces the dimensionality of the data while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: (150, 4)\n",
      "Reduced Shape: (150, 2)\n",
      "Explained Variance Ratio: [0.92461872 0.05306648]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize and fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Original Shape:\", X.shape)\n",
    "print(\"Reduced Shape:\", X_pca.shape)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `PCA` transforms the data into principal components. We use `fit_transform` to reduce dimensionality.\n",
    "\n",
    "### 7.3 Model Pipelines\n",
    "\n",
    "Pipelines streamline the workflow by chaining preprocessing steps and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with scaling and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Preprocessing step\n",
    "    ('classifier', LogisticRegression())  # Model\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Pipeline Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `Pipeline` allows chaining preprocessing and modeling steps. It simplifies the workflow and ensures that preprocessing is applied consistently.\n",
    "\n",
    "## 8. Saving and Loading Models\n",
    "\n",
    "Saving and loading models is essential for deploying and reusing trained models.\n",
    "\n",
    "### 8.1 Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: `joblib.dump` saves the trained model to a file.\n",
    "### 8.2 Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Predict using the loaded model\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Loaded Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: joblib.load loads a saved model from a file, allowing you to use it for predictions or further evaluation.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We covered supervised and unsupervised learning algorithms using the Iris dataset and demonstrated how to preprocess data, build models, and evaluate them. We also looked at saving and loading models for practical applications. Each function and concept was introduced with code examples, making it easier to understand their usage and significance in machine learning workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
